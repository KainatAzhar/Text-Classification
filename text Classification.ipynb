{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text Classification with a Fine-Tuned BERT Model\n",
    "\n",
    "### Project Overview\n",
    "This project showcases a modern approach to Natural Language Processing (NLP) by fine-tuning a pre-trained **BERT (Bidirectional Encoder Representations from Transformers)** model for a specific text classification task. Unlike traditional models that rely on hand-crafted features, BERT learns contextual relationships between words, leading to state-of-the-art performance.\n",
    "\n",
    "### Dataset\n",
    "The model is fine-tuned on the **IMDb movie review dataset**, which contains 50,000 movie reviews labeled as either positive or negative. This dataset is a standard benchmark for sentiment analysis, allowing for the demonstration of a robust classification model.\n",
    "\n",
    "### Methodology\n",
    "1.  **Tokenization and Encoding:** The raw text data is preprocessed using BERT's specialized tokenizer. It converts text into a numerical format that the model can understand, including `input_ids` and `attention_masks`.\n",
    "2.  **Model Loading:** A pre-trained `BertForSequenceClassification` model is loaded from the Hugging Face Transformers library. This powerful base model already possesses a deep understanding of language.\n",
    "3.  **Fine-tuning:** The pre-trained model is fine-tuned on the IMDb dataset. The model's final layers are updated to adapt to the specific sentiment classification task.\n",
    "4.  **Training and Evaluation:** The model is trained for a few epochs with a specialized optimizer (`AdamW`) and a learning rate scheduler, which are best practices for fine-tuning Transformer models. Its performance is evaluated on a validation set.\n",
    "\n",
    "### Concluded Results\n",
    "The fine-tuned BERT model achieves a high classification accuracy (expected to be over 90%), significantly outperforming basic machine learning models. This project demonstrates proficiency in using large language models, a critical skill in modern NLP, and an understanding of advanced training techniques for pre-trained models.\n",
    "\n",
    "### Technologies Used\n",
    "- Python\n",
    "- Hugging Face Transformers\n",
    "- PyTorch\n",
    "- Pandas\n",
    "- Scikit-learn\n",
    "- Jupyter Notebook"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J1r4H1G4B7bO",
    "outputId": "321a4855-4603-490b-d36c-941e737190eb"
   },
   "outputs": [],
   "source": [
    "# Project 4: Advanced Text Classification with a Fine-Tuned BERT Model\n",
    "\n",
    "# --- Section 1: Setup and Data Loading ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Load IMDb dataset from Hugging Face Datasets library\n",
    "print(\"Loading IMDb dataset...\")\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "df_train = dataset['train'].to_pandas()\n",
    "df_test = dataset['test'].to_pandas()\n",
    "\n",
    "# --- Section 2: Tokenization and Encoding ---\n",
    "\n",
    "print(\"Tokenizing and encoding text data...\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def encode_text(texts, labels):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128, \n",
    "            padding='max_length',  \n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_dataset = encode_text(df_train['text'].tolist(), df_train['label'].tolist())\n",
    "test_dataset = encode_text(df_test['text'].tolist(), df_test['label'].tolist())\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- Section 3: Model Fine-tuning ---\n",
    "\n",
    "print(\"Fine-tuning BERT model...\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2, \n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n======== Epoch {epoch + 1} / {epochs} ========')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = [b.to(device) for b in batch]\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'  Average training loss: {avg_train_loss:.2f}')\n",
    "\n",
    "# --- Section 4: Model Evaluation ---\n",
    "\n",
    "print(\"\\nEvaluating the model on the test dataset...\")\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "for batch in test_dataloader:\n",
    "    b_input_ids, b_input_mask, b_labels = [b.to(device) for b in batch]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "flat_predictions = np.argmax(np.concatenate(predictions, axis=0), axis=1)\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")"
   ]
  }
 ]
}
